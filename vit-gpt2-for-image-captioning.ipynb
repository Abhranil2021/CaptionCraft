{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U rouge rouge_score deep-phonemizer","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:02:54.295228Z","iopub.execute_input":"2024-09-28T14:02:54.295518Z","iopub.status.idle":"2024-09-28T14:03:15.161534Z","shell.execute_reply.started":"2024-09-28T14:02:54.295485Z","shell.execute_reply":"2024-09-28T14:03:15.160233Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1) Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datasets\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import io, transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom transformers import VisionEncoderDecoderModel, ViTFeatureExtractor\nfrom transformers import AutoTokenizer, GPT2Config, default_data_collator\n\nfrom rouge import Rouge\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:15.163954Z","iopub.execute_input":"2024-09-28T14:03:15.164375Z","iopub.status.idle":"2024-09-28T14:03:43.697967Z","shell.execute_reply.started":"2024-09-28T14:03:15.164328Z","shell.execute_reply":"2024-09-28T14:03:43.697182Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.699045Z","iopub.execute_input":"2024-09-28T14:03:43.699646Z","iopub.status.idle":"2024-09-28T14:03:43.704052Z","shell.execute_reply.started":"2024-09-28T14:03:43.699612Z","shell.execute_reply":"2024-09-28T14:03:43.703064Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 2) GPU Check","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"GPU available, GPU count:\", torch.cuda.device_count())\n    print(\"GPU in use:\", torch.cuda.get_device_name())\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, using CPU\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.706747Z","iopub.execute_input":"2024-09-28T14:03:43.707124Z","iopub.status.idle":"2024-09-28T14:03:43.798673Z","shell.execute_reply.started":"2024-09-28T14:03:43.707079Z","shell.execute_reply":"2024-09-28T14:03:43.797808Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"GPU available, GPU count: 1\nGPU in use: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3) Loading Dataset","metadata":{}},{"cell_type":"markdown","source":"## 3.1) Hyperparameters","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (224, 224)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.799655Z","iopub.execute_input":"2024-09-28T14:03:43.799936Z","iopub.status.idle":"2024-09-28T14:03:43.804327Z","shell.execute_reply.started":"2024-09-28T14:03:43.799907Z","shell.execute_reply":"2024-09-28T14:03:43.803518Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 3.2) Transforms\n\nTransforms applied are: Resize -> Convert to Tensor -> Divide by 255.0 (to bring all pixel values between 0 and 1)","metadata":{}},{"cell_type":"code","source":"transformations = transforms.Compose(\n    [\n        transforms.Resize(IMG_SIZE),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x / 255.0)\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.805493Z","iopub.execute_input":"2024-09-28T14:03:43.805752Z","iopub.status.idle":"2024-09-28T14:03:43.815148Z","shell.execute_reply.started":"2024-09-28T14:03:43.805723Z","shell.execute_reply":"2024-09-28T14:03:43.814190Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 3.3) Load the Data, Split into Train and Test Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/flickr8k/captions.txt')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.816020Z","iopub.execute_input":"2024-09-28T14:03:43.816374Z","iopub.status.idle":"2024-09-28T14:03:43.876998Z","shell.execute_reply.started":"2024-09-28T14:03:43.816319Z","shell.execute_reply":"2024-09-28T14:03:43.876032Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size = 0.2)\nprint(\"Number of examples in training data:\", train_df.shape[0])\nprint(\"Number of examples in validation data:\", val_df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.878012Z","iopub.execute_input":"2024-09-28T14:03:43.878309Z","iopub.status.idle":"2024-09-28T14:03:43.889016Z","shell.execute_reply.started":"2024-09-28T14:03:43.878278Z","shell.execute_reply":"2024-09-28T14:03:43.887978Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Number of examples in training data: 3200\nNumber of examples in validation data: 800\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3.4) Load Image Feature Extractor and Text Tokenizer","metadata":{}},{"cell_type":"code","source":"ENCODER = 'google/vit-base-patch16-224'\nDECODER = 'gpt2'","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.890381Z","iopub.execute_input":"2024-09-28T14:03:43.890743Z","iopub.status.idle":"2024-09-28T14:03:43.896194Z","shell.execute_reply.started":"2024-09-28T14:03:43.890703Z","shell.execute_reply":"2024-09-28T14:03:43.895310Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# helper function for building special tokens during caption tokenization\n\ndef build_inputs_with_special_tokens(self, token_ids_0, token_ids_1 = None):\n    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n    return outputs\n\nAutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.900205Z","iopub.execute_input":"2024-09-28T14:03:43.900524Z","iopub.status.idle":"2024-09-28T14:03:43.907385Z","shell.execute_reply.started":"2024-09-28T14:03:43.900478Z","shell.execute_reply":"2024-09-28T14:03:43.906509Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained(ENCODER)\ntokenizer = AutoTokenizer.from_pretrained(DECODER)\ntokenizer.pad_token = tokenizer.unk_token","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:43.908529Z","iopub.execute_input":"2024-09-28T14:03:43.909352Z","iopub.status.idle":"2024-09-28T14:03:47.879281Z","shell.execute_reply.started":"2024-09-28T14:03:43.909318Z","shell.execute_reply":"2024-09-28T14:03:47.878369Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe2f4e385b934e32815ba7d8f31f2edd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a722acf91524c629982bcd09e9bf367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef27bc6f661642d5bf9fdf500cfae361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450b47690c874eaeb9b0cb0486bf068f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c5abed8cdb45daa1f4c85b5fbbfe37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a1b714971242258866d9d5c9df5d9a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3.5) Build Custom Dataset Class","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, df, root_dir, tokenizer, feature_extractor, transform = None):\n        self.df = df \n        self.transform = transform\n        self.root_dir = root_dir\n        self.tokenizer= tokenizer\n        self.feature_extractor = feature_extractor\n        self.max_length = 64\n        \n    def __len__(self,):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        caption = self.df.caption.iloc[idx]\n        image = self.df.image.iloc[idx]\n        img_path = os.path.join(self.root_dir, image)\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform is not None:\n            img = self.transform(img)\n            \n        pixel_values = self.feature_extractor(img, return_tensors = \"pt\").pixel_values\n        captions = self.tokenizer(caption, \n                                  padding = 'max_length', \n                                  max_length = self.max_length).input_ids\n        \n        captions = [caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions]\n        \n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:47.880608Z","iopub.execute_input":"2024-09-28T14:03:47.880988Z","iopub.status.idle":"2024-09-28T14:03:47.890967Z","shell.execute_reply.started":"2024-09-28T14:03:47.880943Z","shell.execute_reply":"2024-09-28T14:03:47.890174Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 3.5) Create Train and Validation Dataset Classes","metadata":{}},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/flickr8k/Images'","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:47.892198Z","iopub.execute_input":"2024-09-28T14:03:47.892880Z","iopub.status.idle":"2024-09-28T14:03:47.900938Z","shell.execute_reply.started":"2024-09-28T14:03:47.892832Z","shell.execute_reply":"2024-09-28T14:03:47.900050Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageDataset(train_df, \n                             root_dir = ROOT_DIR,\n                             tokenizer = tokenizer,\n                             feature_extractor = feature_extractor, \n                             transform = transformations)\n\nval_dataset = ImageDataset(val_df, \n                           root_dir = ROOT_DIR,\n                           tokenizer = tokenizer,\n                           feature_extractor = feature_extractor, \n                           transform = transformations)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:47.902061Z","iopub.execute_input":"2024-09-28T14:03:47.902713Z","iopub.status.idle":"2024-09-28T14:03:47.910146Z","shell.execute_reply.started":"2024-09-28T14:03:47.902671Z","shell.execute_reply":"2024-09-28T14:03:47.909354Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 4) Loading the Model","metadata":{}},{"cell_type":"markdown","source":"## 4.1) Initialization","metadata":{}},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(ENCODER, DECODER)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:47.911336Z","iopub.execute_input":"2024-09-28T14:03:47.911788Z","iopub.status.idle":"2024-09-28T14:03:53.693576Z","shell.execute_reply.started":"2024-09-28T14:03:47.911745Z","shell.execute_reply":"2024-09-28T14:03:53.692796Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"babceb25710146869fb4e3dfd6422706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ac66491b1ea494fafcf4d358cdd0e43"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43317df15df042f8848094bd10198575"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f24d7646cd453b9d3c3f988176add2"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 4.2) Configuring the Model","metadata":{}},{"cell_type":"code","source":"model.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.max_length = 128\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:53.694630Z","iopub.execute_input":"2024-09-28T14:03:53.694910Z","iopub.status.idle":"2024-09-28T14:03:53.700828Z","shell.execute_reply.started":"2024-09-28T14:03:53.694880Z","shell.execute_reply":"2024-09-28T14:03:53.699899Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# 5) Training the Model","metadata":{}},{"cell_type":"markdown","source":"## 5.1) Initializing Hyperparameters","metadata":{}},{"cell_type":"code","source":"TRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 8\nLR = 1e-5\nEPOCHS = 5","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:53.701906Z","iopub.execute_input":"2024-09-28T14:03:53.702213Z","iopub.status.idle":"2024-09-28T14:03:53.715421Z","shell.execute_reply.started":"2024-09-28T14:03:53.702181Z","shell.execute_reply":"2024-09-28T14:03:53.714583Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## 5.2) Configuring Training Arguments","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir = 'VIT_large_gpt2',\n    per_device_train_batch_size = TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size = VAL_BATCH_SIZE,\n    predict_with_generate = True,\n    evaluation_strategy = \"epoch\",\n    do_train = True,\n    do_eval = True,\n#     logging_steps = 1024,  \n#     save_steps = 2048, \n#     warmup_steps = 1024,  \n    learning_rate = LR,\n    num_train_epochs = EPOCHS, \n    overwrite_output_dir = True,\n    save_total_limit = 1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:53.716474Z","iopub.execute_input":"2024-09-28T14:03:53.716729Z","iopub.status.idle":"2024-09-28T14:03:53.749905Z","shell.execute_reply.started":"2024-09-28T14:03:53.716701Z","shell.execute_reply":"2024-09-28T14:03:53.749107Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5.3) Defining Evaluation Metric\n\nEvaluation metric used is Rouge-L ","metadata":{}},{"cell_type":"code","source":"rouge = Rouge()\n\ndef compute_metrics(pred):\n    label_ids = pred.label_ids\n    pred_ids = pred.predictions\n    \n    # remove unnecessary tokens\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens = True)\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens = True)\n    \n    rouge_scores = rouge.get_scores(pred_str, label_str)[0]['rouge-l']\n    \n    return {\n        'rouge-l precision': round(rouge_scores['p'], 4),\n        'rouge-l recall': round(rouge_scores['r'], 4),\n        'rouge-l fmeasure': round(rouge_scores['f'], 4)\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:53.751052Z","iopub.execute_input":"2024-09-28T14:03:53.751440Z","iopub.status.idle":"2024-09-28T14:03:53.757975Z","shell.execute_reply.started":"2024-09-28T14:03:53.751399Z","shell.execute_reply":"2024-09-28T14:03:53.757185Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 5.4) Training the Model Using Seq2Seq Trainer","metadata":{}},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    tokenizer = feature_extractor,\n    model = model,\n    args = training_args,\n    compute_metrics = compute_metrics,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    data_collator = default_data_collator\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:03:53.759057Z","iopub.execute_input":"2024-09-28T14:03:53.759367Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='678' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 678/2000 15:37 < 30:32, 0.72 it/s, Epoch 1.69/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge-l precision</th>\n      <th>Rouge-l recall</th>\n      <th>Rouge-l fmeasure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.991393</td>\n      <td>0.153800</td>\n      <td>0.461500</td>\n      <td>0.230800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 128, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.save_model('VIT_large_gpt2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6) Inference","metadata":{}},{"cell_type":"code","source":"def generate_captions(img, length = 100):\n    features = feature_extractor(img, return_tensors = \"pt\").pixel_values.to(device)\n    encodings = model.generate(features)[0]\n    generated_caption = tokenizer.decode(encodings)\n    trunc_caption = '\\033[96m' + generated_caption[ : length] + '\\033[0m'\n    return trunc_caption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg').convert('RGB')\nimg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_captions(img, length = 150)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}